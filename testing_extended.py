import redis
import logging
import json
import datetime
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup as bs
from textblob import TextBlob as tb
import urllib
from selenium import webdriver
from time import sleep
import threading
from random import randint

url_patern = 'http://tappedout.net/mtg-decks'
# generated by test.py ()
df = pd.read_csv('commander_deck_names.csv')
test = ['/the-notion-is-elemental-my-dear-watson/']

def parser_decks(deck_name, drive, url = url_patern):
    url_comm = url+deck_name

    drive.get(url_comm)
    request = drive.execute_script("window.scrollTo(0,document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;")
    sleep(randint(6,8))# running 5 strings in tandem time requirements should be round 10hours.will likely run 10+ trings.
    returns = bs(drive.page_source, 'html.parser')

    # this section gets the subcontainers within tapped outs deck container sction.
    temp1 = []
    temp = returns.findAll('ul')
    for i in temp:# crrent issue is here<<<<< my have failled to get the right tags.
        temp1.append(i.findAll('a', href = re.compile('/mtg-card/')))


    # results =  com_result + deck_result
    # following section grabs the list of lists genereated by the above and cleans it up into a single list
    # working on also sifting out the commander name... 
    fresults = []
    comm = []
    for i in temp1:
        set_ = clean(i)
        # if set_ == None:
        #     comm.extend(clean(i, comm_trig = True))
        # else:
        fresults.extend(clean(i))
        
    # then it all gets bundeled up in dictionaries.... considering turning it to lists for ease of use..(I hate dictionaries....)
    res_dict = {str(comm) : [fresults]}
    return  res_dict

# add following three to tool_belt.py hella usefull in future ventures....
def clean(t_list,comm_trig = False):# cleaning up the name html.
    list_= pd.DataFrame()
    command =[]
    for i in t_list:
        try:
            list_.append(i.get('data-name'))
            if i.get('data-name') == None: 
                #.find('a', attr = 'data-name'))
                command.append(i.get(re.find('a',  'href="/mtg-card/')))

        except: # need to come p with conditional to define the commander..
            #command.append(i.get('href'))
            print('if there are more than 3-4 of these ther is a problem here!!!')
    return command + list_ # remember partner commanders, need to add cleanfor that. 
    
def strip_ (text_):# used in the above .
    step1 = re.sub('<a href=/mtg-card/"','',text_)
    step1 = re.sub('"','',step1)
    step2 = step1.split()
    step3 = step2[0]
    return step3

class spider_Thread(threading.Thread):
    def __init__(self, com_names):
        threading.Thread.__init__(self) 
        self.com_names = com_names
        self.driver = webdriver.Firefox(executable_path=r'/home/ed/Documents/gecko/geckodriver-v0.26.0-linux64/geckodriver')
        self.list_ = {}

    def run(self):
        # driver = webdriver.Firefox(executable_path=r'/home/ed/Documents/gecko/geckodriver-v0.26.0-linux64/geckodriver')
        for i in self.com_names:
            temp = parser_decks(i, self.driver)
            self.list_.append(temp)
        list1 = self.list_ 
        self.driver.quit()
        return list1




test_tread = spider_Thread(test)
result1 = test_tread.run()
print(len(result1[0]), result1)
# /the-notion-is-elemental-my-dear-watson/